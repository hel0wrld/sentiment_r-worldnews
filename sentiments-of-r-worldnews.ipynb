{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-05-03T10:18:09.959865Z","iopub.execute_input":"2023-05-03T10:18:09.960243Z","iopub.status.idle":"2023-05-03T10:18:09.999077Z","shell.execute_reply.started":"2023-05-03T10:18:09.960210Z","shell.execute_reply":"2023-05-03T10:18:09.997601Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/rwordnews-textual/worldnews_text.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### This dataset is a smaller and cleaned version of the main [r/worldnews dataset](https://www.kaggle.com/datasets/butterfly232/reddit-dataset?select=worldnews.csv), which was also collected by me.\n### I have already applied Named Entity Recognition using [SpaCy](https://spacy.io/api/entityrecognizer) which categorizes the text into major entities, such as proper nouns of countries, companies, famous persons etc. This can also understood as categorizing of text into major entities.","metadata":{}},{"cell_type":"code","source":"df= pd.read_csv(\"/kaggle/input/rwordnews-textual/worldnews_text.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-05-03T10:18:10.001035Z","iopub.execute_input":"2023-05-03T10:18:10.001458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### This dataset also has the text bodies fetched and cleaned by the [newspaper3k library](https://newspaper.readthedocs.io/en/latest/). The process takes >3hours so I have used the final obtained dataset. However, the text obtained is not exactly clean and contains escape characters like newline","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Checking a typical text from the text_url","metadata":{}},{"cell_type":"code","source":"df.loc[3]['text_url']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### I have employed [VADER](https://github.com/cjhutto/vaderSentiment) for sentiment analysis. This method gives emphasis on words such as \"very\" or \"much\" and even text emoticons like \":)\", so I have not performed a full text preprocessing and only removed the newlines","metadata":{}},{"cell_type":"code","source":"import nltk\nnltk.download('vader_lexicon')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.sentiment.vader import SentimentIntensityAnalyzer\nsent= SentimentIntensityAnalyzer()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### A typical VADER score consists of negative, positive, neutral and compound. While the rest are self-explanatory, compound shows the magnitude of overall emotion that the text conveys.","metadata":{}},{"cell_type":"code","source":"sent.polarity_scores('Putin launches yet another attack on Ukraine front')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean(text):\n    text= text.replace('\\n', '')\n    return text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['text_clean']= df['text_url'].apply(lambda x: clean(str(x)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['scores']= df['text_clean'].apply(lambda x: sent.polarity_scores(x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['compound']=df['scores'].apply(lambda x:x['compound'])\ndf['pos']=df['scores'].apply(lambda x:x['pos'])\ndf['neg']=df['scores'].apply(lambda x:x['neg'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Here, we have the overall sentiment analysis of article texts attached to the reddit posts on r/worldnews. The NER column contains the entities in which the text can be categorized.","metadata":{}},{"cell_type":"code","source":"df.sort_values(by= ['compound'], ascending= True).head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Above is a typical example of most negative sentiment articles sorted by compound scores.","metadata":{}}]}